{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from typing import Self\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "minimum_profile_length_r: int = 5\n",
    "minimum_profile_length_a: int = 5\n",
    "auth_distance_measure: str = 'r234_a23'\n",
    "\n",
    "# control log level\n",
    "debug: bool = True\n",
    "verbose: bool = False\n",
    "trace: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and read user profiles\n",
    "def keystrokes_to_digraphs(keystroke_array):\n",
    "    digraphs = []\n",
    "    i = 0    \n",
    "    while i < len(keystroke_array) - 1:\n",
    "        digraphs.append((\n",
    "            str(keystroke_array[i][0]) +\"-\"+ str(keystroke_array[i+1][0]),\n",
    "            np.round((keystroke_array[i+1][1]-keystroke_array[i][1]), 5)\n",
    "            ))\n",
    "        i +=1\n",
    "    return digraphs\n",
    "\n",
    "def keystrokes_to_trigraphs(keystroke_array):\n",
    "    trigraphs = []\n",
    "    i = 0\n",
    "    while i < len(keystroke_array) - 2:   \n",
    "        trigraphs.append((\n",
    "            str(keystroke_array[i][0])+\"-\"+str(keystroke_array[i+1][0]) + \"-\" + str(keystroke_array[i+2][0]), \n",
    "            np.round((keystroke_array[i+2][1]-keystroke_array[i][1]), 5)\n",
    "            ))\n",
    "        i +=1\n",
    "    return trigraphs\n",
    "\n",
    "def keystrokes_to_fourgraphs(keystroke_array):\n",
    "    fourgraphs = []\n",
    "    i = 0\n",
    "    while i < len(keystroke_array) -3:\n",
    "        fourgraphs.append((\n",
    "            str(keystroke_array[i][0]) + \"-\" + str(keystroke_array[i+1][0]) + \"-\" + str(keystroke_array[i+2][0]) + \"-\" + str(keystroke_array[i+3][0]), \n",
    "            np.round((keystroke_array[i+3][1] - keystroke_array[i][1]), 5)\n",
    "        ))\n",
    "        i += 1\n",
    "    return fourgraphs\n",
    "\n",
    "\n",
    "def calculate_mean_for_duplicates(ngraphs):\n",
    "    cleaned_ngraphs = []\n",
    "    processed_keys = []\n",
    "    for key, time in ngraphs:\n",
    "        if key not in processed_keys:\n",
    "            duplicates = [e for e in ngraphs if e[0] == key ]\n",
    "            if len(duplicates) > 1:\n",
    "                processed_keys.append(key)\n",
    "                cleaned_ngraphs.append((key, np.round(np.mean([d[1] for d in duplicates]), 5)))\n",
    "            else :\n",
    "                processed_keys.append(key)\n",
    "                cleaned_ngraphs.append((key,time))\n",
    "    return cleaned_ngraphs\n",
    "\n",
    "def create_user_profile(keystroke_sequence):\n",
    "    digraphs = calculate_mean_for_duplicates(keystrokes_to_digraphs(keystroke_sequence))\n",
    "    trigraphs = calculate_mean_for_duplicates(keystrokes_to_trigraphs(keystroke_sequence))\n",
    "    fourgraphs = calculate_mean_for_duplicates(keystrokes_to_fourgraphs(keystroke_sequence))\n",
    "    return digraphs, trigraphs, fourgraphs\n",
    "\n",
    "\n",
    "def read_file(complete: pd.DataFrame, user: int, set: int) -> list[(str, int)]:\n",
    "    key_codes = complete.loc[(complete['user'] == user) & (complete['set'] == set)]['key'].to_list()\n",
    "    timestamps =complete.loc[(complete['user'] == user) & (complete['set'] == set)]['timestamp'].to_list()\n",
    "\n",
    "    keystrokes = [(str(k), t) for (k,t) in zip(key_codes, timestamps)]\n",
    "\n",
    "    return keystrokes\n",
    "\n",
    "def read_user_data(complete):\n",
    "    users = []\n",
    "    \n",
    "    for user in range(1, 32):\n",
    "        tmp_keystrokes = []\n",
    "        for set in range(1, 16):\n",
    "            f = read_file(complete, user, set)\n",
    "            tmp_keystrokes.append(f)\n",
    "        users.append(tmp_keystrokes)\n",
    "    return users\n",
    "\n",
    "def get_user_profiles(user_data):\n",
    "    user_profiles = []\n",
    "    count = 0\n",
    "    for u_data in user_data:\n",
    "        digraphs = []\n",
    "        trigraphs =[]\n",
    "        fourgraphs = []\n",
    "        for sample in u_data:\n",
    "            tmp_digraphs, tmp_trigraphs, tmp_fourgraphs = create_user_profile(sample)\n",
    "            digraphs.append(dict(tmp_digraphs))\n",
    "            trigraphs.append(dict(tmp_trigraphs))\n",
    "            fourgraphs.append(dict(tmp_fourgraphs))\n",
    "\n",
    "        user_profiles.append({\"digraphs\": digraphs, \"trigraphs\": trigraphs, \"fourgraphs\": fourgraphs})\n",
    "        count += 1\n",
    "    return user_profiles\n",
    "\n",
    "def create_user_profiles(path_to_userdata, filename):\n",
    "    user_data2 = read_user_data(pd.read_csv(path_to_userdata))\n",
    "    user_profiles = get_user_profiles(user_data2)\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        pickle.dump(user_profiles, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstractions\n",
    "class Sample:\n",
    "    def __init__(self, digraphs: dict[str, float], trigraphs: dict[str, float], fourgraphs: dict[str, float]):\n",
    "        self.digraphs = digraphs\n",
    "        self.trigraphs = trigraphs\n",
    "        self.fourgraphs = fourgraphs\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'digraphs: {self.digraphs} trigraphs: {self.trigraphs} fourgraphs: {self.fourgraphs}'\n",
    "\n",
    "    def get_intersection(self, other: Self) -> Self:\n",
    "        intersection_digraphs = self.digraphs.keys() & other.digraphs.keys()\n",
    "        intersection_trigraphs = self.trigraphs.keys() & other.trigraphs.keys()\n",
    "        intersection_fourgraphs = self.fourgraphs.keys() & other.fourgraphs.keys()\n",
    "\n",
    "\n",
    "        s_digraphs = {k: v for k, v in self.digraphs.items() if k in intersection_digraphs}\n",
    "        s_trigraphs = {k: v for k, v in self.trigraphs.items() if k in intersection_trigraphs}\n",
    "        s_fourgraphs = {k: v for k, v in self.fourgraphs.items() if k in intersection_fourgraphs}\n",
    "\n",
    "        return Sample(s_digraphs, s_trigraphs, s_fourgraphs)\n",
    "\n",
    "    def get_digraphs(self) -> dict[str, float]:\n",
    "        return self.digraphs\n",
    "    \n",
    "    def get_trigraphs(self) -> dict[str, float]:\n",
    "        return self.trigraphs\n",
    "    \n",
    "    def get_fourgraphs(self) -> dict[str, float]:\n",
    "        return self.fourgraphs\n",
    "\n",
    "\n",
    "class UserProfile:\n",
    "    def __init__(self, profile: dict[str, list[dict]]):\n",
    "\n",
    "        assert len(profile['digraphs']) == len(profile['trigraphs']) == len(profile['fourgraphs'])\n",
    "        \n",
    "        self.digraphs = profile['digraphs']\n",
    "        self.trigraphs = profile['trigraphs']\n",
    "        self.fourgraphs = profile['fourgraphs']\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'digraphs: {self.digraphs} trigraphs: {self.trigraphs} fourgraphs: {self.fourgraphs}'\n",
    "\n",
    "    def get_sample_count(self) -> int:\n",
    "        return len(self.digraphs)\n",
    "\n",
    "    def get_sample(self, index: int) -> Sample:\n",
    "        return Sample(self.digraphs[index], self.trigraphs[index], self.fourgraphs[index])\n",
    "    \n",
    "    def get_without_sample(self, index: int) -> Self:\n",
    "        out = deepcopy(self)\n",
    "        \n",
    "        del out.digraphs[index]\n",
    "        del out.trigraphs[index]\n",
    "        del out.fourgraphs[index]\n",
    "        return out\n",
    "\n",
    "    def get_samples(self) -> list[Sample]:\n",
    "        out = [self.get_sample(i) for i in range(self.get_sample_count())]\n",
    "        return out\n",
    "\n",
    "    def m(self) -> float:\n",
    "        samples: list[Sample] = self.get_samples()\n",
    "\n",
    "        distances: dict[str, list[float]] = defaultdict(list)\n",
    "\n",
    "        # calculate distances from each set in profile\n",
    "        for i, sample_A in enumerate(samples):\n",
    "            for j, sample_B in enumerate(samples):\n",
    "                # distance from same sample does not have to be calculated\n",
    "                if j == i:\n",
    "                    assert sample_A == sample_B\n",
    "                    continue\n",
    "                \n",
    "                distance_combinations: dict[str, float] = d(sample_A, sample_B)\n",
    "\n",
    "                # append each distance to distances\n",
    "                for key, value in distance_combinations.items():\n",
    "                    distances[key].append(value)\n",
    "\n",
    "        # calculate mean for each distance and return\n",
    "        return {k: np.array(v).mean() for k, v in distances.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic distances\n",
    "def a_distance(sample_A_ngraphs: dict[str, float], sample_B_ngraphs: dict[str, float], threshold: float = 1.05) -> float:\n",
    "\n",
    "    assert len(sample_A_ngraphs) == len(sample_B_ngraphs)\n",
    "\n",
    "\n",
    "    # check that a minimal number of digraphs are shared\n",
    "    number_of_shared_ngraphs = len(sample_A_ngraphs)\n",
    "    if number_of_shared_ngraphs < minimum_profile_length_a:\n",
    "        if trace: print(f'[TRACE]: Insufficient number of n-graphs: {number_of_shared_ngraphs}')\n",
    "        return 1\n",
    "    \n",
    "    similar_ngraphs:int = 0\n",
    "\n",
    "    # for each n-graph\n",
    "    for n_graph in sample_A_ngraphs:\n",
    "\n",
    "        d1: float = sample_A_ngraphs[n_graph]\n",
    "        d2: float = sample_B_ngraphs[n_graph]\n",
    "\n",
    "        # if distance for two inputs is 0, \n",
    "        # set two very small number, to prevent division by 0\n",
    "        # TODO: is this ok?\n",
    "        if d1 == 0:\n",
    "            d1 = 0.0000001\n",
    "        \n",
    "        if d2 == 0:\n",
    "            d2 = 0.0000001\n",
    "\n",
    "        # 1 < max(d1, d2)/min(d1, d2) ≤ t\n",
    "        if 1 < max(d1, d2) / min(d1, d2) <= threshold:\n",
    "            similar_ngraphs += 1\n",
    "            \n",
    "    distance:float = 1 - (similar_ngraphs / number_of_shared_ngraphs)\n",
    "\n",
    "    return np.round(distance, 6)\n",
    "\n",
    "def r_distance(sample_A_ngraphs: dict[str, float], sample_B_ngraphs: dict[str, float]) -> float:\n",
    "    assert len(sample_A_ngraphs) == len(sample_B_ngraphs)\n",
    "\n",
    "    # check that a minimal number of digraphs are shared\n",
    "    number_of_shared_ngraphs = len(sample_A_ngraphs)\n",
    "    if number_of_shared_ngraphs < minimum_profile_length_a:\n",
    "        if trace: print(f'[TRACE]: Insufficient number of n-graphs: {number_of_shared_ngraphs}')\n",
    "        return 1\n",
    "\n",
    "\n",
    "    # order refernce(user profile) n-graphs based on n-grpah duration\n",
    "    sample_A_ngraphs_sorted = list(dict(sorted(sample_A_ngraphs.items(), key= lambda item: item[1])))\n",
    "   \n",
    "    # order sample n-graphs based on n-grpah duration\n",
    "    sample_B_ngraphs_sorted = list(dict(sorted(sample_B_ngraphs.items(), key= lambda item: item[1])))\n",
    "    \n",
    "    # calculate distances between n-graph positions in reference and evaluation datasets\n",
    "    ordered_distances = [abs(sample_A_ngraphs_sorted.index(ele) - idx) for idx, ele in enumerate(sample_B_ngraphs_sorted)]\n",
    "    \n",
    "    # calculate maximum degree of disorder\n",
    "    # (if |V| is even) 0> (|V|^2 / 2)\n",
    "    if number_of_shared_ngraphs % 2 == 0:\n",
    "        maximum_disorder = ((number_of_shared_ngraphs * number_of_shared_ngraphs)) / 2\n",
    "    # (if |V| is odd) => (|V|^2 − 1) / 2\n",
    "    else:\n",
    "        maximum_disorder = ((number_of_shared_ngraphs * number_of_shared_ngraphs) - 1) / 2\n",
    "\n",
    "    # calculate r-distance\n",
    "    distance = np.sum(ordered_distances) / maximum_disorder\n",
    "\n",
    "    return np.round(distance, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic d(distance) and md(mean distance) functions\n",
    "def d(sample_A: Sample, sample_B: Sample) -> dict[str, float]:\n",
    "\n",
    "\n",
    "    # get shared n-graphs\n",
    "    shared_sample_A = sample_A.get_intersection(sample_B)\n",
    "    shared_sample_B = sample_B.get_intersection(sample_A)\n",
    "\n",
    "    assert shared_sample_A.get_digraphs().keys() == shared_sample_B.get_digraphs().keys()\n",
    "    assert shared_sample_A.get_trigraphs().keys() == shared_sample_B.get_trigraphs().keys()\n",
    "    assert shared_sample_A.get_fourgraphs().keys() == shared_sample_B.get_fourgraphs().keys()\n",
    "\n",
    "\n",
    "    # get basic distances\n",
    "    a2 = a_distance(shared_sample_A.get_digraphs(), shared_sample_B.get_digraphs())\n",
    "    a3 = a_distance(shared_sample_A.get_trigraphs(), shared_sample_B.get_trigraphs())\n",
    "    a4 = a_distance(shared_sample_A.get_fourgraphs(), shared_sample_B.get_fourgraphs())\n",
    "\n",
    "    r2 = r_distance(shared_sample_A.get_digraphs(), shared_sample_B.get_digraphs())\n",
    "    r3 = r_distance(shared_sample_A.get_trigraphs(), shared_sample_B.get_trigraphs())\n",
    "    r4 = r_distance(shared_sample_A.get_fourgraphs(), shared_sample_B.get_fourgraphs())\n",
    "\n",
    "    # will contain all combinations of a- and r-distances\n",
    "    out: dict[str, float] = {}\n",
    "\n",
    "    out['a2'] = a2\n",
    "    out['a3'] = a3\n",
    "    out['a4'] = a4\n",
    "\n",
    "    out['r2'] = r2\n",
    "    out['r3'] = r3\n",
    "    out['r4'] = r4\n",
    "\n",
    "    out['a23'] =  a2 + a3\n",
    "    out['a34'] =  a3 + a4\n",
    "    out['a234'] = a2 + a3 + a4\n",
    "\n",
    "    out['r23'] =  r2 + r3\n",
    "    out['r34'] =  r3 + r4\n",
    "    out['r234'] = r2 + r3 + r4\n",
    "\n",
    "    out['r2_a2'] = r2 + a2\n",
    "    out['r3_a3'] = r3 + a3\n",
    "    out['r4_a4'] = r4 + a4\n",
    "\n",
    "    out['r23_a23'] = r2 + r3 + a2 + a3\n",
    "    out['r34_a34'] = r3 + r4 + a3 + a4\n",
    "    out['r24_a24'] = r2 + r4 + a2 + a4\n",
    "\n",
    "    out['r234_a234'] = r2 + r3 + r4 + a2 + a3 + a4\n",
    "\n",
    "    out['r2_a3'] = r2 + a3\n",
    "    out['r2_a4'] = r2 + a4\n",
    "\n",
    "    out['r3_a2'] = r3 + a2\n",
    "    out['r4_a2'] = r4 + a2\n",
    "\n",
    "    out['r23_a2'] = r2 + r3 + a2\n",
    "    out['r23_a3'] = r2 + r3 + a3\n",
    "    out['r23_a4'] = r2 + r3 + a4\n",
    "\n",
    "    out['r234_a2'] = r2 + r3 + r4 + a2\n",
    "    out['r234_a3'] = r2 + r3 + r4 + a3\n",
    "    out['r234_a4'] = r2 + r3 + r4 + a4\n",
    "\n",
    "    out['r234_a23'] =  r2 + r3 + r4 + a2 + a3\n",
    "\n",
    "    out['r2_a234'] = r2 + a2 + a3 + a4\n",
    "    out['r3_a234'] = r3 + a2 + a3 + a4\n",
    "    out['r4_a234'] = r4 + a2 + a3 + a4\n",
    "\n",
    "    out['r23_a234'] =  r2 + r3 + a2 + a3 + a4\n",
    "    out['r34_a234'] =  r3 + r4 + a2 + a3 + a4\n",
    "\n",
    "    return out\n",
    "\n",
    "def md(user: UserProfile, sample: Sample) -> dict[str, float]:\n",
    "    '''\n",
    "    Calculates the mean distances between the user profile and the sample.\n",
    "\n",
    "            Parameters:\n",
    "                    user (UserProfile): A user profile to calculate the distance to\n",
    "                    sample (Sample): A sample to calculate the distance from\n",
    "\n",
    "            Returns:\n",
    "                    index (dict[str, float]): The mean distance combinations\n",
    "    '''\n",
    "    assert isinstance(user, UserProfile), f'Wrong input type: {type(user)}'\n",
    "    assert isinstance(sample, Sample), f'Wrong input type: {type(sample)}'\n",
    "\n",
    "\n",
    "\n",
    "    distances: dict[str, list[float]] = defaultdict(list)\n",
    "\n",
    "    # calculate distance to each set from user profile\n",
    "    for user_sample in user.get_samples():\n",
    "        distance_combinations: dict[str, float] = d(user_sample, sample)\n",
    "\n",
    "        # append each distance to distances\n",
    "        for key, value in distance_combinations.items():\n",
    "            distances[key].append(value)\n",
    "\n",
    "    # calculate mean for each distance and return\n",
    "    mean_distances: dict[str, float] = {k: np.array(v).mean() for k, v in distances.items()}\n",
    "    return mean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user classification\n",
    "def user_classification(distances: dict[str, list[float]], distance_measure: str) -> int:\n",
    "    \n",
    "    # will contains the r234_a23 distance for each user \n",
    "    user_distances = [user_distance for user_distance in distances[distance_measure]]\n",
    "\n",
    "    # returns the index(user) with the minimal distance\n",
    "    return np.argmin(user_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentification legal\n",
    "def authentication_test_legal_connection(user_profiles_training: list[UserProfile], user_profiles_evaluation: list[UserProfile]) -> tuple[int, int]:\n",
    "    # keep track of results\n",
    "    false_reject_classification: int = 0\n",
    "    false_reject_distance: int = 0\n",
    "    attempt: int = 0\n",
    "\n",
    "    assert type(user_profiles_evaluation) == list,  type(user_profiles_evaluation)\n",
    "\n",
    "    # for each user, try to authenticate with each sample\n",
    "    for test_user_index, test_user in enumerate(user_profiles_evaluation):\n",
    "        print(\"Start legal attempt user \" + str(test_user_index))\n",
    "\n",
    "\n",
    "        assert type(test_user) == UserProfile,  type(test_user)\n",
    "\n",
    "        # try to classify user with each set as test sample\n",
    "        for sample_index in range(0, test_user.get_sample_count()):\n",
    "            attempt += 1\n",
    "\n",
    "            # get sample to test\n",
    "            test_sample = test_user.get_sample(sample_index)\n",
    "\n",
    "            # remove test sample from profile\n",
    "            # test_profile_removed = test_user.get_without_sample(sample_index)\n",
    "\n",
    "            # create copy of list and swap test user profile\n",
    "            #user_profiles = deepcopy(user_profiles_training)\n",
    "            #user_profiles[test_user_index] = test_profile_removed\n",
    "\n",
    "            user_profiles = user_profiles_training\n",
    "\n",
    "            # calculate distances from sample to user profiles\n",
    "            distances: list[dict[str, float]] = [md(user_profile, test_sample) for user_profile in user_profiles]\n",
    "\n",
    "            # convert list of dicts to dict of lists\n",
    "            distances_c: dict[str, list[float]] = defaultdict(list)\n",
    "            for entry in distances:\n",
    "                for key, value in entry.items():\n",
    "                    distances_c[key].append(value)\n",
    "            \n",
    "            # process calculated distances\n",
    "            classified_user_index = user_classification(distances_c, auth_distance_measure)\n",
    "\n",
    "\n",
    "            # if wrongly classified, continue with next set\n",
    "            if classified_user_index != test_user_index:\n",
    "                false_reject_classification += 1\n",
    "                continue\n",
    "\n",
    "            # classification was successful\n",
    "            else:\n",
    "                if trace: print(f\"[TRACE] Sucessfully classified: Set {sample_index}\")\n",
    "\n",
    "                # \n",
    "                m_A = test_user.m()[auth_distance_measure]\n",
    "                md_A_X = distances[test_user_index][auth_distance_measure]\n",
    "\n",
    "                # check, that distance is close enought\n",
    "                for user_B_distances in distances:\n",
    "                    md_B_X = user_B_distances[auth_distance_measure]\n",
    "\n",
    "                    # md(A, X) < m(A) + 0.5 * (md(B,X) - m(A))\n",
    "                    if md_A_X >= m_A + 0.5 * (md_B_X - m_A):\n",
    "                        false_reject_distance += 1\n",
    "                        if verbose: print(\"[WARN ] Authentification failure: distance too big\")\n",
    "                        break\n",
    "                else:\n",
    "                    if verbose: print(f\"[INFO ] Successfully authenticated: Set {sample_index}\")   \n",
    "\n",
    "\n",
    "    print(\"Attempts: \"+ str(attempt) + \" FR_Classification: \"+ str(false_reject_classification) + \" FR_Distance: \" + str(false_reject_distance))\n",
    "    return (attempt, false_reject_classification + false_reject_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentification imposter\n",
    "def authentication_test_attack(attacked_user_profiles: list[UserProfile], attacker_user_profiles: list[UserProfile]) -> tuple[int, int]:\n",
    "    false_accept: int = 0\n",
    "    attempts: int = 0\n",
    "\n",
    "    # with each sample from attacker, try to authenticate as each user\n",
    "    for attacker_user_index, attacker_user in enumerate(attacker_user_profiles):\n",
    "        print(f\"Start attack from user: {attacker_user_index}\")\n",
    "        for attacked_user_index, attacked_user in enumerate(attacked_user_profiles):\n",
    "            if verbose: print(f\"Attacking user: {attacked_user_index}\")\n",
    "\n",
    "            # user should't attack himself\n",
    "            if attacked_user_index == attacker_user_index: continue\n",
    "\n",
    "            # try to authenticate on attacked profile with each sample from attacker\n",
    "            for attacker_sample_index, attacker_sample in enumerate(attacker_user.get_samples()):\n",
    "                if verbose: print(f\"Attack using set: {attacker_sample_index}\")\n",
    "                attempts += 1 \n",
    "\n",
    "                # calculate distances from sample to user profiles\n",
    "                distances: list[dict[str, float]] = [md(user_profile, attacker_sample) for user_profile in attacked_user_profiles]\n",
    "\n",
    "                # transform from list of dicts to dict of lists\n",
    "                distances_c: dict[str, list[float]] = defaultdict(list)\n",
    "                for entry in distances:\n",
    "                    for key, value in entry.items():\n",
    "                        distances_c[key].append(value)\n",
    "\n",
    "                # find user with smallest distance\n",
    "                classified_user_index = user_classification(distances_c, auth_distance_measure)\n",
    "\n",
    "                # if wrongly classified, continue with next sample\n",
    "                if classified_user_index != attacked_user_index:\n",
    "                    if verbose: print(\"[WARN ] Authentification failure: classification failed\")\n",
    "                    continue\n",
    "\n",
    "                # classification was successful\n",
    "                else:\n",
    "                    if trace: print(f\"[TRACE] Sucessfully classified:\\n  Attacker: {attacker_user_index} Set:{attacker_sample_index} Attacked: {attacked_user_index}\")\n",
    "\n",
    "                    # \n",
    "                    m_A = attacked_user.m()[auth_distance_measure]\n",
    "                    md_A_X = distances[attacked_user_index][auth_distance_measure]\n",
    "\n",
    "                    sucess = False\n",
    "                    # check, that distance is close enought\n",
    "                    for user_B_distances in distances:\n",
    "                        md_B_X = user_B_distances[auth_distance_measure]\n",
    "\n",
    "                        # md(A, X) < m(A) + 0.5 * (md(B,X) - m(A))\n",
    "                        if md_A_X >= m_A + 0.5 * (md_B_X - m_A):\n",
    "                            if verbose: print(\"[WARN ] Authentification failure: distance too big\")\n",
    "                            sucess = True\n",
    "                            break\n",
    "                    if sucess:\n",
    "                        false_accept += 1\n",
    "                        if verbose: print(f\"[INFO ] Successfully authenticated:\\n    Attacker: {attacker_user_index} Set:{attacker_sample_index} Attacked: {attacked_user_index}\")   \n",
    "    \n",
    "    return (attempts, false_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate experiment\n",
    "def authenticate_dataset(path_to_dataset_training: str, path_to_dataset_evaluation: str, output: str, filter:list = []):\n",
    "    # open training data set\n",
    "    with open(path_to_dataset_training, \"rb\") as fp:\n",
    "        user_profiles_training = pickle.load(fp)\n",
    "\n",
    "    # open eval data sets\n",
    "    with open(path_to_dataset_evaluation, \"rb\") as fp:\n",
    "        user_profiles_evaluation = pickle.load(fp)\n",
    "\n",
    "    # remove row 13, 18, 26\n",
    "    user_profiles_training = [UserProfile(j) for i, j in enumerate(user_profiles_training) if i not in filter]\n",
    "    user_profiles_evaluation = [UserProfile(j) for i, j in enumerate(user_profiles_evaluation) if i not in filter]\n",
    "\n",
    "    # try legitimate auth\n",
    "    legal_attempts = authentication_test_legal_connection(user_profiles_training, user_profiles_evaluation)\n",
    "\n",
    "    # try fraudulent auth\n",
    "    attacks = authentication_test_attack(user_profiles_training, user_profiles_evaluation)\n",
    "    \n",
    "    # write results to file\n",
    "    df = pd.DataFrame(data={\n",
    "        'Type':['False Reject', 'False Accept'], \n",
    "        'Attempts':[legal_attempts[0], attacks[0]], \n",
    "        'Result':[legal_attempts[1], attacks[1]]\n",
    "    })\n",
    "    df.to_csv('./__DATA/' + output + \"_authentication.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# users -> sample -> key -> data\n",
    "results_class_global:list[list[dict[str, tuple[int,int]]]] = []\n",
    "results_md_global:list[dict[str, float]] = []\n",
    "\n",
    "\n",
    "\n",
    "def classification_experiment(user_profiles_training: list[UserProfile], user_profiles_evaluation: list[UserProfile]) -> list[dict[str, int]]:\n",
    "\n",
    "    users_scores: list[dict[str, int]] = []\n",
    "\n",
    "    # for each user / userprofile := (digraphs, trigraphs, fourgraphs)\n",
    "    for test_user_index, test_user in enumerate(user_profiles_evaluation):\n",
    "        print(f\"Start classifcation for user {test_user_index}\")\n",
    "\n",
    "\n",
    "        user_score: dict[str, int] = defaultdict(int)\n",
    "        if debug: results_class_global.append([])\n",
    "\n",
    "        # try to classify user with each set as test sample\n",
    "        for sample_index in range(0, test_user.get_sample_count()):\n",
    "\n",
    "            if debug: results_class_global[test_user_index].append({})\n",
    "\n",
    "            # get sample to test\n",
    "            test_sample = test_user.get_sample(sample_index)\n",
    "\n",
    "            # remove test sample from profile\n",
    "            # test_profile_removed = test_user.get_without_sample(sample_index)\n",
    "\n",
    "            # create copy of list and swap test user profile\n",
    "            #user_profiles = deepcopy(user_profiles_training)\n",
    "            #user_profiles[test_user_index] = test_profile_removed\n",
    "\n",
    "            user_profiles: list[UserProfile] = user_profiles_training\n",
    "\n",
    "            # calculate distances from sample to user profiles\n",
    "            distances: list[dict[str, float]] = [md(user_profile, test_sample) for user_profile in user_profiles]\n",
    "\n",
    "            results_md_global = distances\n",
    "\n",
    "            distances_c: dict[str, list[float]] = defaultdict(list)\n",
    "            for entry in distances:\n",
    "                for key, value in entry.items():\n",
    "                    distances_c[key].append(value)\n",
    "\n",
    "            for key in distances_c.keys():\n",
    "                # process calculated distances\n",
    "                classified_user_index = user_classification(distances_c, key)\n",
    "\n",
    "                if classified_user_index == test_user_index:\n",
    "                    user_score[key] += 1\n",
    "                else:\n",
    "                    if verbose: print(f\"[DEBUG] Classification failure {key}: user: {test_user_index} sample: {sample_index} classified user: {classified_user_index}\")\n",
    "                    if debug: results_class_global[test_user_index][sample_index][key] = classified_user_index\n",
    "\n",
    "\n",
    "        users_scores.append(user_score)\n",
    "\n",
    "    return users_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification experiment\n",
    "def classify_dataset(path_to_dataset_training: str, path_to_dataset_evaluation: str, output: str, filter:list = list(range(31))):\n",
    "    # open training data set\n",
    "    with open(path_to_dataset_training, \"rb\") as fp:\n",
    "        user_profiles_training = pickle.load(fp)\n",
    "\n",
    "    # open eval data sets\n",
    "    with open(path_to_dataset_evaluation, \"rb\") as fp:\n",
    "        user_profiles_evaluation = pickle.load(fp)\n",
    "\n",
    "    # map raw profile data to UserProfile\n",
    "    user_profiles_training = [UserProfile(p) for i,p in enumerate(user_profiles_training) if i in filter]\n",
    "    user_profiles_evaluation = [UserProfile(p) for i,p in enumerate(user_profiles_evaluation) if i in filter]\n",
    "\n",
    "\n",
    "    results: list[dict[str, int]] = classification_experiment(user_profiles_training, user_profiles_evaluation)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # get sum of correct classifications\n",
    "    sums = df.sum().to_frame(name=\"Sucessfull\")\n",
    "\n",
    "    # total number of tried classifications\n",
    "    total = 15 * len(user_profiles_evaluation)\n",
    "\n",
    "    # calculate missclassification as (total - sucessfull classifications)\n",
    "    sums[\"Missclassifications\"] = total - sums.iloc[:,0]\n",
    "\n",
    "    # calculate error rate\n",
    "    sums[\"Error\"] = (sums[\"Missclassifications\"] / total) * 100\n",
    " \n",
    "    # write results to file\n",
    "    sums.to_csv('./__DATA/'+ output + '_classification_performance.csv')\n",
    "    df.to_csv('./__DATA/' + output + '_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in original data\n",
    "original_set = './../freetext/FreeText-Dataset-31-USERS.csv'\n",
    "original_data_profiles = './__DATA/original_data_profiles'\n",
    "\n",
    "if not os.path.isfile(original_data_profiles):\n",
    "    create_user_profiles(original_set, original_data_profiles)\n",
    "\n",
    "#classify_dataset(original_data_profiles, original_data_profiles, \"original\")\n",
    "#authenticate_dataset(original_data_profiles, original_data_profiles, \"original\", filter=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromium remote otc ws\n",
    "remote_otc_ws_set = \"./otc_ws/complete.csv\"\n",
    "remote_otc_ws_data_profiles = \"./__DATA/chromium_remote_otc_ws_profiles\"\n",
    "\n",
    "if not os.path.isfile(remote_otc_ws_data_profiles):\n",
    "    create_user_profiles(remote_otc_ws_set, remote_otc_ws_data_profiles)\n",
    "\n",
    "#classify_dataset(remote_otc_ws_data_profiles, remote_otc_ws_data_profiles, \"chromium_remote_otc_ws\")\n",
    "#authenticate_dataset(remote_otc_ws_data_profiles, remote_otc_ws_data_profiles, \"chromium_remote_otc_ws\", filter=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromium remote aws ws\n",
    "remote_aws_ws_set = \"./aws_ws/complete.csv\"\n",
    "remote_aws_ws_data_profiles = \"./__DATA/chromium_remote_aws_ws_profile\"\n",
    "\n",
    "\n",
    "if not os.path.isfile(remote_aws_ws_data_profiles):\n",
    "    create_user_profiles(remote_aws_ws_set, remote_aws_ws_data_profiles)\n",
    "\n",
    "#classify_dataset(remote_aws_ws_data_profiles, remote_aws_ws_data_profiles, \"chromium_remote_aws_ws\")\n",
    "#authenticate_dataset(remote_aws_ws_data_profiles, remote_aws_ws_data_profiles, \"chromium_remote_aws_ws\", filter=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromium remote aws http\n",
    "remote_aws_http_set = \"./aws_http/complete.csv\"\n",
    "remote_aws_http_data_profiles = \"./__DATA/chromium_remote_aws_http_profile\"\n",
    "\n",
    "\n",
    "if not os.path.isfile(remote_aws_http_data_profiles):\n",
    "    create_user_profiles(remote_aws_http_set, remote_aws_http_data_profiles)\n",
    "\n",
    "#classify_dataset(remote_aws_http_data_profiles, remote_aws_http_data_profiles, \"chromium_remote_aws_http\")\n",
    "#authenticate_dataset(remote_aws_http_data_profiles, remote_aws_http_data_profiles, \"chromium_remote_aws_http\", filter=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromium remote otc http\n",
    "remote_otc_http_set = \"./otc_http/complete.csv\"\n",
    "remote_otc_http_data_profiles = \"./__DATA/chromium_remote_otc_http_profile\"\n",
    "\n",
    "\n",
    "if not os.path.isfile(remote_otc_http_data_profiles):\n",
    "    create_user_profiles(remote_otc_http_set, remote_otc_http_data_profiles)\n",
    "\n",
    "#classify_dataset(remote_otc_http_data_profiles, remote_otc_http_data_profiles, \"chromium_remote_otc_http\")\n",
    "#authenticate_dataset(remote_otc_http_data_profiles, remote_otc_http_data_profiles, \"chromium_remote_otc_http\", filter=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chromium isolated\n",
    "isolated_set = \"./complete_chromium_isolated.csv\"\n",
    "isolated_data_profiles = \"./__DATA/chromium_isolated_profiles\"\n",
    "\n",
    "\n",
    "if not os.path.isfile(isolated_data_profiles):\n",
    "    create_user_profiles(isolated_set, isolated_data_profiles)\n",
    "\n",
    "#classify_dataset(isolated_data_profiles, isolated_data_profiles, \"chromium_isolated\")\n",
    "#authenticate_dataset(isolated_data_profiles, isolated_data_profiles, \"chromium_isolated\", filter=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
